{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6038a4f",
   "metadata": {},
   "source": [
    "# Experiment 2\n",
    "\n",
    "To validate our results in a more complex setting, we examine how each distance measure ranks an expert annotation against a single other high-quality candidate repair found by a state-of-the-art automated repair technique. \n",
    "\n",
    "We use the state-of-the-art semantic Automated Repair Tool (ART) Refactory to find a candidate repair for each incorrect solution in our annotated dataset. To obtain a high-quality repair, we run the ART giving it access to the same pool of candidate repairs as used in the first experiment (without the expert solution). Using this pool of correct programs, Refactory generates a bigger suite of semantically equivalent code by refactoring all these available working solutions to a problem. Then, given an incorrect program, Refactory analyzes its control flow structure to find a closely matching working program to compare for isolating the buggy components of the buggy solution. As such, the candidate repair generated by Refactory should be better or at least as appropriate as the best candidates in the original pool (which, once again, might contain the student's own correction to the problem).\n",
    "\n",
    "We repeat the previous experiment (experiment 1) using the candidate repair found for each buggy solution. The main difference with the first experiment is that we compare the expert annotation/repair against the single candidate obtained using Refactory. Therefore, the ranking error for each buggy program becomes a binary classification error. We report the total classification error --  the number of times the ART candidate repair was favored over the expert annotation -- for all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import disable_caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "disable_caching()\n",
    "sns.set_theme(\"paper\")\n",
    "plt.rcParams['font.size'] = '7'\n",
    "sns.set(font_scale=1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.common import dist_funcs, new_assignments_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's load our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = '../configs/conf.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.files import read_config\n",
    "\n",
    "config = read_config(CONFIG_PATH)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Refactory results dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_index(file_name):\n",
    "    return int(file_name.split(\"_\")[-1][:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279ea095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import warn\n",
    "\n",
    "questions = os.listdir(config.save_path)\n",
    "questions = [q for q in questions if q.startswith(\"question\")]\n",
    "key_f = lambda q: int(q.split('_')[-1])\n",
    "questions = sorted(questions, key=key_f)\n",
    "\n",
    "dataframe = []\n",
    "for q in questions:\n",
    "    q_path = os.path.join(config.save_path, q, 'refactory_online.csv')\n",
    "    if not os.path.exists(q_path):\n",
    "        warn(f\"Results for assignment {q} are not available\")\n",
    "        continue\n",
    "    dataframe.append(pd.read_csv(q_path))\n",
    "    \n",
    "dataframe = pd.concat(dataframe, axis=0, ignore_index=True)\n",
    "dataframe[\"index\"] = dataframe[\"File Name\"].apply(extract_index).astype(int)\n",
    "dataframe = dataframe.set_index(\"index\")\n",
    "dataframe = dataframe.sort_index()\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataframe used to obtain the Refactory's repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(os.path.join(config.save_path, 'hgf'))\n",
    "original_df = dataset.to_pandas()\n",
    "# We only take the incorrect ones\n",
    "original_df = original_df[~original_df.correct]\n",
    "original_df = original_df.set_index(\"submission_id\")\n",
    "original_df = original_df.sort_index()\n",
    "original_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge these together.\n",
    "Both dataframe and the original dataset should have the same lenght. Are there mismatches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df = pd.concat([dataframe, original_df], axis=1)\n",
    "results_df = results_df.replace(new_assignments_id)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to remove the results for the reverse_recur assignment since we have only 5 annotations for this one (not enough to matter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df[results_df.assignment_id != \"reverse_recur\"]\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a look at how well Refactory really performs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rexecuting the codes and  looking at what is the real success percentage\n",
    "\n",
    "We notice that Refactory sometimes produces incorrect results but the tool classifies them as correct.\n",
    "To avoid that, let's determine correctness ourselves. We'll only analyze the Results of Refactory on the codes\n",
    "which were successfully corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.TestResults import TestResults\n",
    "\n",
    "results_df.loc[pd.isnull(results_df.Repair), \"Repair\"] = \"\"\n",
    "results_df = TestResults().get_correctness(results_df, \"Repair\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = results_df.groupby(\"assignment_id\")\n",
    "success_percentage = groups.apply(lambda gdf: (gdf.correct.sum() / len(gdf)) * 100)\n",
    "success_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_working = results_df[~results_df.correct]\n",
    "non_working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the distance computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.code import clean_code\n",
    "\n",
    "results_df = results_df[results_df.correct] # take only the Refactory corrections which are actually correct\n",
    "rename = {\n",
    "    \"func_code\": \"buggy_code\",\n",
    "    \"Repair\": \"candidate_code\",\n",
    "    \"annotation\": \"expert_code\"\n",
    "}\n",
    "results_df = results_df.rename(columns=rename)\n",
    "results_df = results_df[[\"buggy_code\", \"candidate_code\", \"expert_code\", \"assignment_id\"]]\n",
    "\n",
    "results_df = results_df[results_df.buggy_code.astype(bool)]\n",
    "results_df[\"buggy_code\"] = results_df[\"buggy_code\"].apply(clean_code)\n",
    "\n",
    "results_df = results_df[results_df.expert_code.astype(bool)]\n",
    "results_df[\"expert_code\"] = results_df[\"expert_code\"].apply(clean_code)\n",
    "\n",
    "results_df = results_df[results_df.candidate_code.astype(bool)]\n",
    "results_df[\"candidate_code\"] = results_df[\"candidate_code\"].apply(clean_code)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b, r, e in results_df[results_df.assignment_id == \"maximum\"][[\"buggy_code\", \"candidate_code\", \"expert_code\"]].to_numpy():\n",
    "    print(b)\n",
    "    print(r)\n",
    "    print(e)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance computations between different codes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compute the classification error between the expert annotation and refactory candidate repair\n",
    "\n",
    "Let's compute the number of times where, if we would use the sequence edit distance, or the string edit distance, we would select the candidate repair (the Refactory output) over the true goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from itertools import product, combinations\n",
    "\n",
    "get_name = lambda c: c.split('_')[0]\n",
    "from_to = list(combinations([\"buggy_code\", \"expert_code\", \"candidate_code\"], 2))\n",
    "elements = list(product(from_to, dist_funcs))\n",
    "for (from_, target), dist_f in elements:\n",
    "    col_name = f\"{dist_f.__name__}-{get_name(from_)}_{get_name(target)}\"\n",
    "    buggies = results_df[from_].to_list()\n",
    "    corrections = results_df[target].to_list()\n",
    "    results_df[col_name] = list(map(dist_f, buggies, corrections))\n",
    "\n",
    "results_df = results_df.reset_index(drop=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error(sub_df):\n",
    "    r = {}\n",
    "    for dist_n in dist_names:\n",
    "        bcd = sub_df[f\"{dist_n}-buggy_candidate\"]\n",
    "        bed = sub_df[f\"{dist_n}-buggy_expert\"]\n",
    "        r[dist_n] = sub_df[bcd < bed].shape[0]\n",
    "               \n",
    "    return pd.Series(r)\n",
    "     \n",
    "\n",
    "dist_names = [d.__name__ for d in dist_funcs]\n",
    "targets = [c.split('_')[0] for c in [\"candidate\", \"expert\"]]\n",
    "dist_names, targets\n",
    "\n",
    "error = results_df.groupby(\"assignment_id\").apply(compute_error)\n",
    "\n",
    "error.columns = [c.replace(\"_dist\", '').upper() for c in error.columns]\n",
    "error = error.sort_values(by=error.first_valid_index(), ascending=False, axis=1)\n",
    "\n",
    "\n",
    "selected_columns = [c for c in error.columns if \"RPS\" not in c]\n",
    "selected_columns = [\"TED\", \"SEQ\", \"STR\", \"TED_NORM\", \"SEQ_NORM\", \"STR_NORM\",\"BLEU\", \"CODEBLEU\", \"ROUGE1\", \"ROUGELCSUM\"]\n",
    "error = error[selected_columns]\n",
    "\n",
    "# adding the number of solutions per assignment as well as the success percentage\n",
    "nb_code = results_df.groupby(\"assignment_id\").buggy_code.count()\n",
    "nb_code.name = \"#prog\"\n",
    "error = pd.concat([nb_code, error], axis=1)\n",
    "total = error.sum(axis=0).astype(int)\n",
    "total.name = \"total\"\n",
    "error.loc[\"total\"] = total\n",
    "error = error.astype(int)\n",
    "error = error.rename(columns = {\n",
    "            \"TED\": 'ted', 'SEQ': 'seq', 'STR': 'str',\n",
    "            \"TED_NORM\": \"nted\", \"STR_NORM\": \"nstr\", \"SEQ_NORM\": \"nseq\", \n",
    "            'BLEU': 'bleu', \"CODEBLEU\": \"codebleu\", \"ROUGE1\": \"rouge\", \"ROUGELCSUM\": \"rougeLCS\"})\n",
    "print(error.to_latex(multicolumn=True, multirow=True, column_format='r|c|ccc|ccc|ccc'))\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the number of times were we observe that the rouge distance metric misclassifies our elements is consistantly lower than for the string distance measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the distances a bit deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average distance between buggy->expert, and buggy->candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt the dataframe\n",
    "df = results_df.melt(\n",
    "    id_vars=\"assignment_id\",\n",
    "    var_name=\"measure\",\n",
    "    value_name=\"value\",\n",
    "    value_vars=[c for c in results_df.columns if \"-\" in c])\n",
    "# rename the distance metrics\n",
    "df[\"distance_metric\"] = df[\"measure\"].apply(lambda dm: dm.split(\"-\")[0])\n",
    "df[\"distance_metric\"] = df[\"distance_metric\"].apply(lambda c: c.replace(\"_dist\", '').upper())\n",
    "df[\"from\"] = df[\"measure\"].apply(lambda dm: dm.split(\"-\")[1].split(\"_\")[0])\n",
    "df[\"to\"] = df[\"measure\"].apply(lambda dm: dm.split(\"-\")[1].split(\"_\")[1])\n",
    "df = df.replace({\"ROUGELCSUM\": \"ROUGELCS\"})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.distance_metric.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_univariate(metric):\n",
    "    print(\"Metric\", metric)\n",
    "    sub_df = df[(df.distance_metric == metric) & (df[\"from\"] == \"buggy\")]\n",
    "    g = sns.displot(data=sub_df, x=\"value\", hue=\"to\", col=\"distance_metric\", kde=True)\n",
    "    sns.move_legend(g, \"center\", bbox_to_anchor=(0.50, 0.65), ncol=11, title=None, frameon=True)\n",
    "    plt.savefig(f'images/{metric}_hist.pdf', dpi=100,  bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ecdf(metric):\n",
    "    sub_df = df[(df.distance_metric == metric) & (df[\"from\"] == \"buggy\")]\n",
    "    g = sns.displot(data=sub_df, x=\"value\", hue=\"to\", kind=\"ecdf\", col=\"distance_metric\")\n",
    "    sns.move_legend(g, \"center\", bbox_to_anchor=(0.50, 0.30), ncol=11, title=None, frameon=True)\n",
    "    plt.savefig(f'images/{metric}_ecdf.pdf', dpi=100,  bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in [\"STR\", \"SEQ\", \"ROUGELCS\", \"SEQ_NORM\", \"STR_NORM\"]:\n",
    "    plot_univariate(metric)\n",
    "    plot_ecdf(metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (feedback)",
   "language": "python",
   "name": "feedback"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
