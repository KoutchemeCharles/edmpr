{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f1d093b",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "\n",
    "We compare different distance metrics using the following strategy: for each buggy solution for each assignment in our dataset, we create a pool of potential repair candidates using automatic repair tools, and we include our expert annotation in that pool. Following previous work~\\cite{gupta_neural_attr}, we include all correct programs submitted by all students for a particular assignment across all academic years into the pool. We highlight that this pool might include the buggy solution's author's own written working solution to the exercise (which may or may not be similar to the buggy solution). Then, using the selected distance metric, we compute the distance between the buggy solution and each candidate repair before ranking each candidate solution from worst to best according to how small the distance value is. Finally, we use the position of the expert annotation in the ranking as an error measure, the Ranking Error (RE). For example, the expert solution being ranked first/having the smallest distance has an RE of 0. To account for the different number of candidate repairs per assignment, we normalize the error by the total number of candidates. We refer to this performance measure as the Normalized Ranking Error (NRE) for the single buggy solution. Finally, we report the Average Normalized Ranking Error (ANRE) for each assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4b5b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import disable_caching, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2697990",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_caching()\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "sys.path.append(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.common import dist_funcs, new_assignments_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ef2e60",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f464302",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"koutch/intro_prog\", \"dublin_data\")\n",
    "ori_df = pd.concat([dataset[\"train\"].to_pandas(), dataset[\"test\"].to_pandas()])\n",
    "ori_df = ori_df.replace(new_assignments_id)\n",
    "ori_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The part we annotated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7843ddf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "dataset = load_dataset(\"koutch/intro_prog\", \"dublin_repair\", \n",
    "                       download_mode=\"force_redownload\",\n",
    "                       ignore_verifications=True)\n",
    "dataset = concatenate_datasets(list(dataset.values()))\n",
    "dataframe = dataset.to_pandas()\n",
    "dataframe = dataframe[dataframe.annotation.astype(bool)]\n",
    "dataframe = dataframe[dataframe.func_code.astype(bool)]\n",
    "dataframe = dataframe.replace(new_assignments_id)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add back the user_ids to the repair_dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_to_user = ori_df[[\"submission_id\", \"user\"]].set_index(\"submission_id\")[\"user\"].to_dict()\n",
    "dataframe[\"user\"] = [sub_to_user[uid] for uid in dataframe[\"submission_id\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7685de5",
   "metadata": {},
   "source": [
    "## Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28869caa",
   "metadata": {},
   "source": [
    "#### Let's look at some distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb7c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = dataframe[[\"func_code\", \"annotation\"]].to_numpy()\n",
    "\n",
    "for dist_f in dist_funcs:\n",
    "    dataframe[dist_f.__name__] = [dist_f(b, c) for b, c in pairs]\n",
    "    sns.displot(data=dataframe, x=dist_f.__name__, hue=\"func_name\", kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926d5349",
   "metadata": {},
   "source": [
    "#### Ranking the expert annotaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315bb824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expert_relative_position(dataframe, ori_df, dist_f, norm=True):\n",
    "    \"\"\" Obtain the relative position of the expert annotation\n",
    "    according to the given distance metric. \"\"\"\n",
    "    \n",
    "    ranks = []\n",
    "    columns = [\"func_code\", \"annotation\", \"assignment_id\"]\n",
    "    for bc, annot, aid in dataframe[columns].to_numpy():\n",
    "        # Take all the correct solutions submitted to this assignment \n",
    "        correct_solutions = ori_df[(ori_df.assignment_id == aid) \n",
    "                                   & (ori_df.correct)].func_code.unique()\n",
    "        correct_solutions = correct_solutions.tolist()\n",
    "        # Add the expert annotation \n",
    "        correct_solutions = [annot] + correct_solutions\n",
    "        \n",
    "        # Compute the distance to each potential candidate and\n",
    "        # rank the candidates based on how close they are to the\n",
    "        # buggy code according to the distance metric\n",
    "        \n",
    "        \n",
    "        distances = [dist_f(bc, c) for c in correct_solutions]\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        # we return the absolute position of the expert annotation\n",
    "        # we look for the position of the added annotation (last position)\n",
    "        rank = list(sorted_indices).index(0)\n",
    "        rank = rank / len(sorted_indices) if norm else rank \n",
    "        ranks.append(rank)\n",
    "        \n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757f130f",
   "metadata": {},
   "source": [
    "#### Ranking the student own correct submission\n",
    "\n",
    "We also take a look at the same thing, but using the student own submitted correct solution later. In that way, we are kind of looking at whether student own submitted solutions align with the expert annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c189047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_first_correct(df):\n",
    "    \"\"\" \n",
    "    To each submission by a user, assign a correction\n",
    "    as being the first submitted correct code by that user.\n",
    "    \"\"\"\n",
    "    \n",
    "    first_correct = df[df.correct].sort_values(by=\"date\")\n",
    "    if first_correct.empty:\n",
    "        df[\"first_user_correction\"] = \"\"\n",
    "    else:\n",
    "        df[\"first_user_correction\"] = first_correct.func_code.iloc[0]\n",
    "    return df\n",
    "\n",
    "def soc_rank(dataframe, ori_df, dist_f, norm=True):\n",
    "    \"\"\" \n",
    "    Obtain the relative position of the student own\n",
    "    submitted correct solution against other solutions. \n",
    "    \"\"\"\n",
    "    \n",
    "    fuc = (ori_df.groupby([\"user\", \"assignment_id\"])\n",
    "           .apply(assign_first_correct)\n",
    "           .set_index(\"submission_id\")\n",
    "           .loc[dataframe.submission_id, \"first_user_correction\"]\n",
    "           .reset_index(drop=True))\n",
    "    \n",
    "    dataframe[\"first_user_correction\"] = fuc\n",
    "    \n",
    "    ranks = []\n",
    "    columns = [\"func_code\", \"first_user_correction\", \"assignment_id\"]\n",
    "    # TODO: ensure \n",
    "    # first, we must find for each student, their own last submitteds solution\n",
    "    for bc, annot, aid in dataframe[columns].to_numpy():\n",
    "        correct_solutions = ori_df[(ori_df.assignment_id == aid) \n",
    "                                   & (ori_df.correct)].func_code.unique()\n",
    "        correct_solutions = correct_solutions.tolist()\n",
    "        correct_solutions = [annot] + correct_solutions\n",
    "\n",
    "                \n",
    "        distances = [dist_f(bc, c) for c in correct_solutions]\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        \n",
    "        rank = list(sorted_indices).index(0)\n",
    "        rank = rank / len(sorted_indices) if norm else rank \n",
    "        ranks.append(rank)\n",
    "        \n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d1a810",
   "metadata": {},
   "source": [
    "#### Ranking all students submissions in order\n",
    "\n",
    "The extension to the previous approach is to consider all the students submissions in order, and look\n",
    "at which distance metric allows the retrieval of these.\n",
    "\n",
    "For this, we are going to use the [bpref](https://trec.nist.gov/pubs/trec16/appendices/measures.pdf) metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d8584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_correct(df):\n",
    "    \"\"\" \n",
    "    To each submission by a user, assign a correction\n",
    "    as being the first submitted correct code by that user.\n",
    "    \"\"\"\n",
    "    \n",
    "    tmp = df.sort_values(by=\"date\")\n",
    "    df[\"correct_submissions\"] = \"-|-\".join(tmp.loc[tmp.correct, \"func_code\"].to_list())\n",
    "    \n",
    "    return df\n",
    "\n",
    "def compute_bpref(ranking, rel_docs, irrel_docs):\n",
    "    \"\"\" \n",
    "    Commputes the bpref measure from a ranking\n",
    "    of relevant and irelevant docuements. \n",
    "    \n",
    "    BPref is a function of how frequently relevant documents\n",
    "    are retrieved before non-relevant documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    R, N = len(rel_docs), len(irrel_docs)\n",
    "    det = min(R, N)\n",
    "    \n",
    "    \n",
    "    # we must identify the top r irelevant documents\n",
    "    rel_mask = np.isin(ranking, rel_docs)\n",
    "    irr_mask = np.isin(ranking, irrel_docs)\n",
    "    irr_mask = np.cumsum(irr_mask) < R\n",
    "    ranking = list(np.array(ranking)[(irr_mask | rel_mask)])\n",
    "    \n",
    "    bpref = np.array([1 - (np.isin(ranking[:ranking.index(rd)], irrel_docs).sum() / det)\n",
    "                      for rd in rel_docs]).mean()\n",
    "    \n",
    "    return bpref \n",
    "         \n",
    "        \n",
    "    \n",
    "def asoc_bpref_rank(dataframe, ori_df, dist_f):\n",
    "    \"\"\" \n",
    "    Obtain a measure of the student own\n",
    "    submitted correct solution against other solutions.\n",
    "    \"\"\"\n",
    "        \n",
    "    f = (ori_df.groupby([\"user\", \"assignment_id\"])\n",
    "         .apply(find_all_correct)\n",
    "         .set_index(\"submission_id\")\n",
    "         .loc[dataframe.submission_id, \"correct_submissions\"]\n",
    "         .reset_index(drop=True))\n",
    "    \n",
    "    dataframe[\"correct_submissions\"] = f\n",
    "    \n",
    "    ranks = []\n",
    "    columns = [\"func_code\", \"correct_submissions\", \"assignment_id\"]\n",
    "    \n",
    "    for bc, cs, aid in dataframe[columns].to_numpy():\n",
    "        correct_solutions = ori_df[(ori_df.assignment_id == aid) \n",
    "                                   & (ori_df.correct)].func_code.unique()\n",
    "        correct_solutions = correct_solutions.tolist()\n",
    "        \n",
    "        student_corr_solutions = cs.split(\"-|-\")\n",
    "        \n",
    "        # Ensure that in the other irelevant solutions there is not the student ones\n",
    "        correct_solutions = list(set(correct_solutions).difference(student_corr_solutions))\n",
    "        # Relevant solutions and irrelevant solutions \n",
    "        correct_solutions = student_corr_solutions + correct_solutions \n",
    "        \n",
    "        # obtain the ranking\n",
    "        \n",
    "        distances = [dist_f(bc, c) for c in correct_solutions]\n",
    "        ranking = list(np.argsort(distances))\n",
    "    \n",
    "        docs = list(range(len(distances)))\n",
    "        rel_docs = docs[:len(student_corr_solutions)]\n",
    "        irrel_docs = docs[len(student_corr_solutions):]\n",
    "                  \n",
    "        rank = compute_bpref(ranking, rel_docs, irrel_docs)\n",
    "        ranks.append(rank)\n",
    "        \n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d28d60e",
   "metadata": {},
   "source": [
    "### Let's obtain all the distance measures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.code import clean_code\n",
    "\n",
    "dataframe[\"func_code\"] = dataframe[\"func_code\"].apply(clean_code)\n",
    "dataframe[\"annotation\"] = dataframe[\"annotation\"].apply(clean_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This bellow will take a bit of time (be patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381eb79c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "ranking_measures = [expert_relative_position] #, soc_rank] #, asoc_bpref_rank]\n",
    "\n",
    "for rank_f, dist_f in product(ranking_measures, dist_funcs):\n",
    "    col_name = f\"{dist_f.__name__}-{rank_f.__name__}\"\n",
    "    dataframe[col_name] = rank_f(dataframe, ori_df, dist_f)\n",
    "    \n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47800c6b",
   "metadata": {},
   "source": [
    "### Plotting time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f26ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframe.melt(\n",
    "    id_vars=[c for c in dataframe.columns if \"rank\" not in c],\n",
    "    var_name=\"ranking\",\n",
    "    value_name=\"ranking_value\",\n",
    "    value_vars=[c for c in dataframe.columns if c.endswith(\"rank\")])\n",
    "\n",
    "aid_to_id = {k: i for i, k in enumerate(sorted(df[\"assignment_id\"].unique()))}\n",
    "df[\"aid\"] = df[\"assignment_id\"].apply(lambda aid: aid_to_id[aid])\n",
    "df[\"distance_metric\"] = df[\"ranking\"].apply(lambda dm: dm.split(\"-\")[0])\n",
    "df[\"ranking_metric\"] = df[\"ranking\"].apply(lambda dm: dm.split(\"-\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd6d1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df.groupby([\"assignment_id\", \"distance_metric\", \"ranking_metric\"]).ranking_value.agg([\"mean\", \"std\"])\n",
    "groups = groups.reset_index()\n",
    "groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the table format a bit to have nice displayable results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_order = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_for_display(table):\n",
    "    #\n",
    "    selected_metrics = [\"rougelcsum_dist\", \"rouge1_dist\", \"bleu_dist\", \"codebleu_dist\",\n",
    "                        \"str_norm_dist\", \"seq_norm_dist\", \"ted_norm_dist\",\n",
    "                        \"str_dist\", \"seq_dist\", \"ted_dist\"]\n",
    "    table = table[table.distance_metric.isin(selected_metrics)]\n",
    "    #\n",
    "    table[\"mean\"] = table[\"mean\"].round(3).apply(lambda v: \"%.3f\" % float(v))\n",
    "    table[\"std\"] = table[\"std\"].round(2).apply(lambda v: \"%.2f\" % float(v))\n",
    "    table = table.fillna(0)\n",
    "    table[\"mean_std\"] = [f\"{mean} (+-{std})\" for mean, std in table[[\"mean\", \"std\"]].to_numpy()]\n",
    "        \n",
    "    bft = table.copy()\n",
    "    rename_cols = lambda columns: [c.replace(\"_dist\", '').upper() for c in columns]\n",
    "    averages = bft.groupby([\"distance_metric\"])[\"mean\"].apply(lambda f: np.mean(list(map(float, f))))\n",
    "    averages.index = rename_cols(averages.index)\n",
    "    \n",
    "    # use here mean_std to get the also the variance \n",
    "    table = table.pivot(columns='distance_metric', index=\"assignment_id\", values=\"mean\")\n",
    "    table = table[selected_metrics[::-1]]\n",
    "    \n",
    "    # Let's rename the columns, we remove the dist and put the metric in upper case\n",
    "    table.columns = rename_cols(table.columns)\n",
    "    \n",
    "    \n",
    "    table.index.name = \"\"\n",
    "    table = table.T\n",
    "    # table = table.sort_values(by=list(table.columns), ascending=False)\n",
    "     \n",
    "    display = lambda t: print(t.to_latex(multicolumn=True, multirow=True, \n",
    "                                         column_format='r' + ('c' * len(t.columns))))\n",
    "    \n",
    "    if reverse_order:\n",
    "        t = table.iloc[:, :len(table.columns) // 2]\n",
    "        display(t)\n",
    "        t = table.iloc[:, len(table.columns) // 2:]\n",
    "        display(t)\n",
    "    else:\n",
    "        table = table.T\n",
    "        # adding the mean value at the bottom\n",
    "        averages = averages.apply(lambda v: \"%.3f\" % float(v))\n",
    "        averages = averages.to_frame().T.round(3)\n",
    "        \n",
    "        table = pd.concat([table, averages], axis=0)\n",
    "        table = table.rename(columns = {\n",
    "            \"TED\": 'ted', 'SEQ': 'seq', 'STR': 'str',\n",
    "            \"TED_NORM\": \"nted\", \"STR_NORM\": \"nstr\", \"SEQ_NORM\": \"nseq\", \n",
    "            'BLEU': 'bleu', \"CODEBLEU\": \"codebleu\", \"ROUGE1\": \"rouge\", \"ROUGELCSUM\": \"rougeLCS\"})\n",
    "        table = table[[\"ted\", \"seq\", \"str\", \"nted\", \"nseq\", \"nstr\", \"bleu\", \"codebleu\", \"rouge\", \"rougeLCS\"]]\n",
    "        display(table)\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = groups[groups.ranking_metric == \"expert_relative_position\"].reset_index(drop=True)\n",
    "table = process_for_display(table)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalized ranking error of the student own solution\n",
    "\n",
    "How do different measures perform for retrieving the students own solutions (\"own repairs\") to the problem. You'll observe that their own solution has a higher error accross all distances compared to experts. In other words, our experts annotations are closer to students code than students own corrections. This makes sense if we consider that students often change strategies while trying to solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = groups[groups.ranking_metric == \"soc_rank\"].reset_index(drop=True)\n",
    "table = process_for_display(table)\n",
    "table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (edmpr)",
   "language": "python",
   "name": "edmpr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
