{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a96e837c",
   "metadata": {},
   "source": [
    "# Evaluating the Refactory Model on the Dublin Program Repair Dataset \n",
    "\n",
    "#### Running this notebook is optional, since we already compiled the data for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import disable_caching\n",
    "\n",
    "disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.files import read_config\n",
    "\n",
    "ref_config = read_config(\"../configs/conf.json\")\n",
    "ref_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "annotated_dataset = load_dataset(\"koutch/intro_prog\", \"dublin_repair\")\n",
    "annotated_dataset = concatenate_datasets(list(annotated_dataset.values())).to_pandas()\n",
    "original_dataset = load_dataset(\"koutch/intro_prog\", \"dublin_data\")\n",
    "original_dataset = concatenate_datasets(list(original_dataset.values())).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_dataset[\"assignment_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset[\"assignment_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from src.common import new_assignments_id\n",
    "\n",
    "annotated_dataset = annotated_dataset[annotated_dataset.annotation.astype(bool)]\n",
    "ann_subids = set(annotated_dataset[\"submission_id\"])\n",
    "original_dataset = original_dataset[original_dataset[\"correct\"]]\n",
    "original_dataset = original_dataset[~original_dataset.submission_id.isin(ann_subids)]\n",
    "assignments = set(annotated_dataset[\"assignment_id\"]).intersection(original_dataset[\"assignment_id\"])\n",
    "original_dataset = original_dataset[original_dataset.assignment_id.isin(assignments)]\n",
    "annotated_dataset = annotated_dataset[annotated_dataset.assignment_id.isin(assignments)]\n",
    "annotated_dataset[\"correct\"] = False\n",
    "\n",
    "dataset = pd.concat([annotated_dataset, original_dataset], axis=0)\n",
    "dataset = dataset.replace(new_assignments_id)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432f3771",
   "metadata": {},
   "source": [
    "Loading the annotations of the buggy code from the online interface. The \"train\" split is a default split, but it it's not meaningful. We need also access to correct solutions. Let's load them from the original data.Moreover, the academic years might have had different assignments. Refactory cannot repair buggy programs if no data about these assignments are available, so we ensure we only keep assignments present accross the two splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "annotated_dataset = load_dataset(\"koutch/intro_prog\", \"dublin_repair\")[\"train\"]\n",
    "annotated_dataset = annotated_dataset.filter(lambda ex: bool(ex[\"annotation\"]))\n",
    "ann_subids = set(annotated_dataset[\"submission_id\"])\n",
    "if ref_config.split_year:\n",
    "    annotated_dataset = annotated_dataset.filter(lambda ex: ex[\"academic_year\"] ==2017)\n",
    "    \n",
    "hgf_dataset = load_dataset(\"koutch/intro_prog\", \"dublin_data\")\n",
    "\n",
    "# We take the correct solutions from the training split (2015-21016)\n",
    "if ref_config.split_year:\n",
    "    original_dataset = hgf_dataset[\"train\"]\n",
    "else:\n",
    "    original_dataset = concatenate_datasets(list(hgf_dataset.values()))\n",
    "    \n",
    "original_dataset = original_dataset.filter(lambda ex: ex[\"correct\"])\n",
    "original_dataset = original_dataset.filter(lambda ex: ex[\"submission_id\"] not in ann_subids)\n",
    "\n",
    "assignments = set(annotated_dataset[\"assignment_id\"]).intersection(original_dataset[\"assignment_id\"])\n",
    "filter_f = lambda ex: ex[\"assignment_id\"] in assignments\n",
    "original_dataset = original_dataset.filter(filter_f)\n",
    "annotated_dataset = annotated_dataset.filter(filter_f)\n",
    "\n",
    "original_dataset, annotated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from datasets import Value\n",
    "\n",
    "def add_correctness(example):\n",
    "    example[\"correct\"] = False\n",
    "    return example\n",
    "    \n",
    "annotated_dataset = annotated_dataset.map(add_correctness)\n",
    "new_features = annotated_dataset.features.copy()\n",
    "new_features[\"academic_year\"] = Value(\"int32\")\n",
    "annotated_dataset = annotated_dataset.cast(new_features)\n",
    "dataset = concatenate_datasets([original_dataset, annotated_dataset])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from src.common import new_assignments_id\n",
    "\n",
    "df = dataset\n",
    "df = df.replace(new_assignments_id)\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7f147a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.refactory import create_save_dir\n",
    "\n",
    "create_save_dir(dataset, ref_config.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.files import create_dir\n",
    "\n",
    "hgf_save_path = os.path.join(ref_config.save_path, 'hgf')\n",
    "# Be careful bellow, it might overide the location\n",
    "# create_dir(hgf_save_path)\n",
    "# dataset.save_to_disk(hgf_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b0a963",
   "metadata": {},
   "source": [
    "To run our evaluation using Refactory, we need to provide the location to a folder where the algorithm is going to read files and perform the corrections. In order to run the code, one should download the [refactory tool online](https://github.com/githubhuyang/refactory) according to their instructions. Then, execute the repair tool by passing by the path to the temporary directory where the data was formatted.\n",
    "\n",
    "\n",
    "```\n",
    "python3 run.py -d ./[PATH_TO_TEMP -q question_1 question_2 question_3 question_4 ... question_10 -s 100 -o -m -b\n",
    "```\n",
    "\n",
    "To look at the results, follow the script refactory_analysis.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (feedback)",
   "language": "python",
   "name": "feedback"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
